import numpy as np
from stable_baselines3.common.callbacks import BaseCallback

class StabilityCallback(BaseCallback):
    """Callback Ð´Ð»Ñ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð° Ð¸ Ð¾Ð±ÐµÑÐ¿ÐµÑ‡ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""

    def __init__(self, check_freq=100, verbose=1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.recent_rewards = []
        self.recent_kl_divs = []
        self.stability_threshold = 2.0  # ÐŸÐ¾Ñ€Ð¾Ð³ Ð½ÐµÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
        self.kl_threshold = 0.1  # ÐŸÐ¾Ñ€Ð¾Ð³ KL-Ð´Ð¸Ð²ÐµÑ€Ð³ÐµÐ½Ñ†Ð¸Ð¸

    def _on_step(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Ð¸Ð· Ð»Ð¾Ð³Ð³ÐµÑ€Ð°
        if hasattr(self.model, 'logger') and self.model.logger.name_to_value:
            logger_dict = self.model.logger.name_to_value
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ KL-Ð´Ð¸Ð²ÐµÑ€Ð³ÐµÐ½Ñ†Ð¸ÑŽ ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°
            kl_div = None
            for key in logger_dict.keys():
                if 'kl_div' in key.lower() or 'approx_kl' in key.lower():
                    kl_div = logger_dict[key]
                    break
            
            if kl_div is not None:
                self.recent_kl_divs.append(kl_div)
                if len(self.recent_kl_divs) > 50:  # Ð¥Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 50 Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹
                    self.recent_kl_divs.pop(0)
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñƒ Ð²Ñ‹Ð·Ð¾Ð²Ð° (ÐºÐ°Ð¶Ð´Ñ‹Ðµ check_freq ÑˆÐ°Ð³Ð¾Ð²)
        if self.n_calls % self.check_freq == 0:
            self._check_stability()
        
        return True

    def _check_stability(self):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð²Ð°Ñ€Ð¸Ð°Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð½Ð°Ð³Ñ€Ð°Ð´
        if hasattr(self, 'logger') and self.logger.name_to_value:
            if 'train/rollout/ep_rew_mean' in self.logger.name_to_value:
                current_reward = self.logger.name_to_value['train/rollout/ep_rew_mean']
                self.recent_rewards.append(current_reward)
                
                if len(self.recent_rewards) > 100:  # Ð¥Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 100 Ð½Ð°Ð³Ñ€Ð°Ð´
                    self.recent_rewards.pop(0)
                
                if len(self.recent_rewards) >= 10:
                    recent_mean = np.mean(self.recent_rewards[-10:])
                    recent_std = np.std(self.recent_rewards[-10:])
                    
                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸ÐµÐ½Ñ‚ Ð²Ð°Ñ€Ð¸Ð°Ñ†Ð¸Ð¸
                    cv = recent_std / abs(recent_mean) if abs(recent_mean) > 0.01 else float('inf')
                    
                    if cv > self.stability_threshold:
                        print(f"âš ï¸  High reward volatility detected! CV: {cv:.2f}")
                        # Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸ÐºÑƒ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸ learning rate
                        self._adjust_learning_rate(0.9)  # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ°ÐµÐ¼ LR Ð½Ð° 10%
                    
                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ KL-Ð´Ð¸Ð²ÐµÑ€Ð³ÐµÐ½Ñ†Ð¸ÑŽ
                    if len(self.recent_kl_divs) >= 5:
                        avg_kl = np.mean(self.recent_kl_divs[-5:])
                        if avg_kl < 0.001:  # ÐžÑ‡ÐµÐ½ÑŒ Ð½Ð¸Ð·ÐºÐ°Ñ KL-Ð´Ð¸Ð²ÐµÑ€Ð³ÐµÐ½Ñ†Ð¸Ñ
                            print(f"âš ï¸  Low KL divergence detected: {avg_kl:.4f}")
                            print("   This indicates insufficient policy updates")
                            self._adjust_learning_rate(1.1)  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ LR Ð½Ð° 10%

    def _adjust_learning_rate(self, factor):
        """ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ learning rate"""
        if hasattr(self.model, 'lr_schedule') and hasattr(self.model.lr_schedule, 'initial_lr'):
            current_lr = self.model.learning_rate
            new_lr = current_lr * factor
            # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½ LR
            new_lr = np.clip(new_lr, 1e-6, 1e-3)
            print(f"   Adjusting learning rate: {current_lr:.6f} -> {new_lr:.6f}")
            
            # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ learning rate (ÑÑ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð² Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð²ÐµÑ€ÑÐ¸ÑÑ… SB3)
            try:
                self.model.learning_rate = new_lr
            except:
                print("   Could not adjust learning rate directly")

    def _on_training_end(self):
        """Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸"""
        if len(self.recent_rewards) >= 50:
            final_cv = np.std(self.recent_rewards[-50:]) / abs(np.mean(self.recent_rewards[-50:]))
            print(f"\nðŸ“Š Final stability check:")
            print(f"   Final coefficient of variation: {final_cv:.2f}")
            if final_cv < 0.5:
                print("   âœ… Good stability achieved!")
            else:
                print("   âš ï¸  High variability in final rewards")


def get_adaptive_lr(initial_lr=3e-5, kl_threshold=0.01):
    """
    Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾Ð³Ð¾ learning rate
    
    Args:
        initial_lr: ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ learning rate
        kl_threshold: ÐŸÐ¾Ñ€Ð¾Ð³ KL-Ð´Ð¸Ð²ÐµÑ€Ð³ÐµÐ½Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸
    """
    def schedule(progress_remaining):
        """
        ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾Ðµ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ learning rate
        
        Args:
            progress_remaining: ÐžÑÑ‚Ð°Ð²ÑˆÐ¸Ð¹ÑÑ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (0-1)
        """
        # Ð‘Ð°Ð·Ð¾Ð²Ð¾Ðµ ÑƒÐ±Ñ‹Ð²Ð°ÑŽÑ‰ÐµÐµ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ
        base_lr = initial_lr * progress_remaining
        
        # Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ Ð»Ð¾Ð³Ð¸ÐºÑƒ Ð°Ð´Ð°Ð¿Ñ‚Ð°Ñ†Ð¸Ð¸ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ KL-Ð´Ð¸Ð²ÐµÑ€Ð³ÐµÐ½Ñ†Ð¸Ð¸
        # ÐŸÐ¾ÐºÐ° Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð±Ð°Ð·Ð¾Ð²Ð¾Ðµ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ
        return base_lr
    
    return schedule