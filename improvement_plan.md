# üìà –ü–ª–∞–Ω –£–ª—É—á—à–µ–Ω–∏—è RL Trading Agent

## üéØ –û–±—â–∞—è –°—Ç—Ä–∞—Ç–µ–≥–∏—è

–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—è–≤–ª–µ–Ω—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏. –î–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ—à–∞–≥–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.

---

## üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–ù–µ–¥–µ–ª—è 1-2)

### 1. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ë–∞–ª–∞–Ω—Å–∞ –õ–æ–Ω–≥/–®–æ—Ä—Ç

**–ü—Ä–æ–±–ª–µ–º–∞:** –ê–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç 0 –ª–æ–Ω–≥-—Å–¥–µ–ª–æ–∫ –∏ 2000 —à–æ—Ä—Ç-—Å–¥–µ–ª–æ–∫ (ratio: 0.00)

**–†–µ—à–µ–Ω–∏—è:**

#### A. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –§—É–Ω–∫—Ü–∏–∏ –ù–∞–≥—Ä–∞–¥—ã (train_rl_balanced.py)
```python
# –î–æ–±–∞–≤–∏—Ç—å –≤ EnhancedTradingEnvironment

def calculate_balanced_reward(self, action, pnl_pct, position_type):
    """
    –†–∞—Å—á–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã —Å —É—á–µ—Ç–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    """
    base_reward = np.log1p(pnl_pct) * 100 if pnl_pct > -0.99 else -100
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    if hasattr(self, 'action_history'):
        recent_actions = self.action_history[-100:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 –¥–µ–π—Å—Ç–≤–∏–π
        long_count = sum(1 for a in recent_actions if a in [1, 2])
        short_count = sum(1 for a in recent_actions if a in [3, 4])
        total_trades = long_count + short_count
        
        if total_trades > 10:
            long_ratio = long_count / total_trades
            # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç 50/50
            balance_penalty = abs(long_ratio - 0.5) * 50  # 0-25
            base_reward -= balance_penalty
    
    return base_reward
```

#### B. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –í—Ä–∞—â–µ–Ω–∏–µ –î–µ–π—Å—Ç–≤–∏–π
```python
# –í TradingEnvironment –¥–æ–±–∞–≤–∏—Ç—å:

class DirectionalBalanceTracker:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –±–∞–ª–∞–Ω—Å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
    
    def __init__(self, window_size=50, target_ratio=0.5, tolerance=0.2):
        self.window_size = window_size
        self.target_ratio = target_ratio
        self.tolerance = tolerance
        self.direction_history = []
    
    def update(self, action):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
        if action in [1, 2]:  # Long
            self.direction_history.append('long')
        elif action in [3, 4]:  # Short
            self.direction_history.append('short')
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞
        if len(self.direction_history) > self.window_size:
            self.direction_history.pop(0)
    
    def get_recommendation(self):
        """–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞"""
        if len(self.direction_history) < 10:
            return None  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        
        long_count = self.direction_history.count('long')
        short_count = self.direction_history.count('short')
        total = long_count + short_count
        
        if total == 0:
            return None
        
        long_ratio = long_count / total
        
        if long_ratio < self.target_ratio - self.tolerance:
            return 'long'  # –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –ª–æ–Ω–≥–æ–≤
        elif long_ratio > self.target_ratio + self.tolerance:
            return 'short'  # –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ —à–æ—Ä—Ç–æ–≤
        
        return None  # –ë–∞–ª–∞–Ω—Å –≤ –Ω–æ—Ä–º–µ
```

#### C. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –û–±—É—á–µ–Ω–∏—è
```python
# –í train_rl_balanced.py:

# 1. –£–≤–µ–ª–∏—á–∏—Ç—å entropy coefficient –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
ent_coef=0.05  # –ë—ã–ª–æ 0.02

# 2. –£–º–µ–Ω—å—à–∏—Ç—å learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
learning_rate=5e-5  # –ë—ã–ª–æ 1e-4

# 3. –î–æ–±–∞–≤–∏—Ç—å curiosity-driven exploration
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

class CuriosityFeatureExtractor(BaseFeaturesExtractor):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —É—á–µ—Ç–æ–º –Ω–æ–≤–∏–∑–Ω—ã —Å–æ—Å—Ç–æ—è–Ω–∏–π"""
    
    def __init__(self, observation_space, features_dim=128):
        super().__init__(observation_space, features_dim)
        self.net = torch.nn.Sequential(
            torch.nn.Linear(observation_space.shape[0], 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, features_dim)
        )
        # –°–µ—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.forward_model = torch.nn.Sequential(
            torch.nn.Linear(features_dim + 5, 256),  # features + action
            torch.nn.ReLU(),
            torch.nn.Linear(256, observation_space.shape[0])
        )
    
    def forward(self, observations):
        return self.net(observations)
```

---

## üü† –í—ã—Å–æ–∫–∏–π –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–ù–µ–¥–µ–ª—è 2-3)

### 2. –£–ª—É—á—à–µ–Ω–∏–µ Win Rate –∏ Profit Factor

**–ü—Ä–æ–±–ª–µ–º–∞:** Win rate 46.7%, Profit factor 0.968 (< 1.0)

**–†–µ—à–µ–Ω–∏—è:**

#### A. –£–ª—É—á—à–µ–Ω–Ω–∞—è –°–∏—Å—Ç–µ–º–∞ –£–ø—Ä–∞–≤–ª–µ–Ω–∏—è –†–∏—Å–∫–∞–º–∏
```python
# –í enhanced_trading_environment.py

class AdaptiveRiskManager:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, base_position_size=0.1, max_position_size=0.5):
        self.base_position_size = base_position_size
        self.max_position_size = max_position_size
        self.volatility_window = []
        self.win_streak = 0
        self.loss_streak = 0
    
    def calculate_position_size(self, atr, current_price, balance):
        """
        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–∑–∏—Ü–∏–∏
        """
        # –ë–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ ATR
        volatility_factor = 1.0 / (1 + atr / current_price * 100)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ—Ä–∏–∏
        streak_factor = 1.0
        if self.win_streak >= 3:
            streak_factor = 1.2  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Å–ª–µ –ø–æ–±–µ–¥
        elif self.loss_streak >= 2:
            streak_factor = 0.7  # –£–º–µ–Ω—å—à–∞–µ–º –ø–æ—Å–ª–µ –ø–æ—Ä–∞–∂–µ–Ω–∏–π
        
        position_size = self.base_position_size * volatility_factor * streak_factor
        return min(position_size, self.max_position_size)
    
    def update_streak(self, pnl):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –ø–æ–±–µ–¥/–ø–æ—Ä–∞–∂–µ–Ω–∏–π"""
        if pnl > 0:
            self.win_streak += 1
            self.loss_streak = 0
        else:
            self.loss_streak += 1
            self.win_streak = 0
```

#### B. –£–ª—É—á—à–µ–Ω–Ω—ã–µ –£—Å–ª–æ–≤–∏—è –í—Ö–æ–¥–∞/–í—ã—Ö–æ–¥–∞
```python
# –î–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –≤—Ö–æ–¥–∞:

def should_enter_long(self, obs):
    """–£—Å–ª–æ–≤–∏—è –¥–ª—è –≤—Ö–æ–¥–∞ –≤ –ª–æ–Ω–≥"""
    indicators = self._extract_indicators(obs)
    
    # –ú–∏–Ω–∏–º—É–º 2 –∏–∑ 3 —É—Å–ª–æ–≤–∏–π:
    conditions = [
        indicators['rsi'] < 40,  # –ü–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å
        indicators['close'] < indicators['bb_lower'],  # –ù–∏–∂–µ –Ω–∏–∂–Ω–µ–π –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞
        indicators['mfi'] < 30,  # MFI –ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω
        indicators['obv_slope'] > 0,  # –†–æ—Å—Ç OBV
    ]
    
    return sum(conditions) >= 2

def should_enter_short(self, obs):
    """–£—Å–ª–æ–≤–∏—è –¥–ª—è –≤—Ö–æ–¥–∞ –≤ —à–æ—Ä—Ç"""
    indicators = self._extract_indicators(obs)
    
    conditions = [
        indicators['rsi'] > 60,  # –ü–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å
        indicators['close'] > indicators['bb_upper'],  # –í—ã—à–µ –≤–µ—Ä—Ö–Ω–µ–π –ë–æ–ª–ª–∏–Ω–¥–∂–µ—Ä–∞
        indicators['mfi'] > 70,  # MFI –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω
        indicators['obv_slope'] < 0,  # –ü–∞–¥–µ–Ω–∏–µ OBV
    ]
    
    return sum(conditions) >= 2
```

#### C. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –°—Ç–æ–ø-–ª–æ—Å—Å—ã –∏ –¢–µ–π–∫-–ø—Ä–æ—Ñ–∏—Ç—ã
```python
# ATR-based —Å—Ç–æ–ø—ã

def calculate_dynamic_stops(self, entry_price, atr, direction):
    """
    –†–∞—Å—á–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å—Ç–æ–ø–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ ATR
    """
    atr_multiplier_sl = 2.0  # –°—Ç–æ–ø-–ª–æ—Å—Å = 2 * ATR
    atr_multiplier_tp = 3.0  # –¢–µ–π–∫-–ø—Ä–æ—Ñ–∏—Ç = 3 * ATR (1:1.5 RR)
    
    if direction == 'long':
        stop_loss = entry_price - atr * atr_multiplier_sl
        take_profit = entry_price + atr * atr_multiplier_tp
    else:
        stop_loss = entry_price + atr * atr_multiplier_sl
        take_profit = entry_price - atr * atr_multiplier_tp
    
    return stop_loss, take_profit
```

---

## üü° –°—Ä–µ–¥–Ω–∏–π –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–ù–µ–¥–µ–ª—è 3-4)

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

#### A. Optuna –¥–ª—è –ü–æ–∏—Å–∫–∞ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
```python
# hyperparameter_optimization.py

import optuna
from stable_baselines3 import PPO

def objective(trial):
    """–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã PPO
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    n_steps = trial.suggest_categorical('n_steps', [512, 1024, 2048, 4096])
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])
    n_epochs = trial.suggest_int('n_epochs', 3, 20)
    gamma = trial.suggest_float('gamma', 0.90, 0.999)
    gae_lambda = trial.suggest_float('gae_lambda', 0.8, 0.99)
    ent_coef = trial.suggest_float('ent_coef', 0.001, 0.1, log=True)
    vf_coef = trial.suggest_float('vf_coef', 0.1, 0.5)
    clip_range = trial.suggest_float('clip_range', 0.1, 0.3)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–±–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=gamma,
        gae_lambda=gae_lambda,
        ent_coef=ent_coef,
        vf_coef=vf_coef,
        clip_range=clip_range,
        verbose=0
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    model.learn(total_timesteps=50000)
    
    # –û—Ü–µ–Ω–∫–∞
    mean_reward = evaluate_model(model, eval_env)
    
    return mean_reward

# –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best params: {study.best_params}")
```

#### B. –°–µ—Ç–æ—á–Ω—ã–π –ü–æ–∏—Å–∫ –¥–ª—è –ö–ª—é—á–µ–≤—ã—Ö –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
```python
# –°–µ—Ç–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
param_grid = {
    'ent_coef': [0.01, 0.02, 0.05, 0.1],  # Exploration
    'learning_rate': [3e-5, 5e-5, 1e-4, 3e-4],
    'n_steps': [512, 1024, 2048],
    'reward_scaling': [0.5, 1.0, 2.0],
    'position_size': [0.05, 0.1, 0.2],
}
```

---

## üü¢ –ù–∏–∑–∫–∏–π –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç (–ù–µ–¥–µ–ª—è 4+)

### 4. –£–ª—É—á—à–µ–Ω–∏–µ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ù–µ–π—Ä–æ—Å–µ—Ç–∏

#### A. Recurrent Policy –¥–ª—è –£—á–µ—Ç–∞ –ò—Å—Ç–æ—Ä–∏–∏
```python
from stable_baselines3.common.policies import RecurrentActorCriticPolicy

model = PPO(
    "MlpLstmPolicy",  # LSTM –¥–ª—è —É—á–µ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    env,
    policy_kwargs=dict(
        lstm_hidden_size=64,
        n_lstm_layers=1,
        enable_critic_lstm=True,
    ),
    verbose=1
)
```

#### B. Ensemble –∏–∑ –ù–µ—Å–∫–æ–ª—å–∫–∏—Ö –ú–æ–¥–µ–ª–µ–π
```python
class EnsembleAgent:
    """–ê–Ω—Å–∞–º–±–ª—å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö RL –∞–≥–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, model_paths):
        self.models = [PPO.load(path) for path in model_paths]
    
    def predict(self, obs, deterministic=True):
        """–ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤"""
        actions = []
        for model in self.models:
            action, _ = model.predict(obs, deterministic=deterministic)
            actions.append(action)
        
        # –ú–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
        return max(set(actions), key=actions.count)
```

---

## üìã –ü–ª–∞–Ω –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### –ù–µ–¥–µ–ª—è 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
- [ ] –î–æ–±–∞–≤–∏—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞–≥—Ä–∞–¥—ã
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å DirectionalBalanceTracker
- [ ] –£–≤–µ–ª–∏—á–∏—Ç—å ent_coef –¥–æ 0.05
- [ ] –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ (100k steps)
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É

### –ù–µ–¥–µ–ª—è 2: –£–ª—É—á—à–µ–Ω–∏–µ –ö–∞—á–µ—Å—Ç–≤–∞
- [ ] –í–Ω–µ–¥—Ä–∏—Ç—å AdaptiveRiskManager
- [ ] –î–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –≤—Ö–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å ATR-based —Å—Ç–æ–ø—ã
- [ ] –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (500k steps)

### –ù–µ–¥–µ–ª—è 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Optuna –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- [ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ —Å–µ—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫
- [ ] –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –ù–µ–¥–µ–ª—è 4: –ü–æ–ª–∏—Ä–æ–≤–∫–∞
- [ ] –í–Ω–µ–¥—Ä–∏—Ç—å Recurrent Policy (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- [ ] –°–æ–∑–¥–∞—Ç—å –∞–Ω—Å–∞–º–±–ª—å –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π
- [ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –£—Å–ø–µ—Ö–∞

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–µ–∫—É—â–µ–µ | –¶–µ–ª–µ–≤–æ–µ |
|---------|---------|---------|
| Balance Score | 0.000 | > 0.5 |
| Long/Short Ratio | 0.00 | 0.3 - 3.0 |
| Win Rate | 46.7% | > 55% |
| Profit Factor | 0.968 | > 1.3 |
| Sharpe Ratio | 9.335 | > 8.0 |
| Max Drawdown | 0.04% | < 5% |

---

## üîÑ –ë—ã—Å—Ç—Ä—ã–µ –ü—Ä–∞–≤–∫–∏ (Immediate Fixes)

### –ü—Ä–∞–≤–∫–∞ 1: –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ù–∞–≥—Ä–∞–¥—ã (train_rl_balanced.py)

```python
# –ù–∞–π—Ç–∏ –∫–ª–∞—Å—Å EnhancedTradingEnvironment –∏ –¥–æ–±–∞–≤–∏—Ç—å:

class EnhancedTradingEnvironment:
    def __init__(self, ...):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        self.action_history = []
        self.long_count = 0
        self.short_count = 0
    
    def step(self, action):
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏
        self.action_history.append(action)
        if action in [1, 2]:
            self.long_count += 1
        elif action in [3, 4]:
            self.short_count += 1
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤ –Ω–∞–≥—Ä–∞–¥—É
        total = self.long_count + self.short_count
        if total > 20:  # –ü–æ—Å–ª–µ 20 —Å–¥–µ–ª–æ–∫ –Ω–∞—á–∏–Ω–∞–µ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å
            long_ratio = self.long_count / total
            balance_bonus = -abs(long_ratio - 0.5) * 10  # -5 to 0
            reward += balance_bonus
        
        return obs, reward, terminated, truncated, info
```

### –ü—Ä–∞–≤–∫–∞ 2: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ

```python
# –í train_rl_balanced.py:

model = PPO(
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ...
    ent_coef=0.05,  # –£–≤–µ–ª–∏—á–∏—Ç—å —Å 0.02
    learning_rate=5e-5,  # –£–º–µ–Ω—å—à–∏—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    # ...
)
```

---

## üéØ –°–ª–µ–¥—É—é—â–∏–µ –î–µ–π—Å—Ç–≤–∏—è

1. **–°–µ–≥–æ–¥–Ω—è:** –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ü—Ä–∞–≤–∫—É 1 –∏ –ü—Ä–∞–≤–∫—É 2
2. **–ó–∞–≤—Ç—Ä–∞:** –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 100k steps
3. **–ß–µ—Ä–µ–∑ 2 –¥–Ω—è:** –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É —Å –ø–æ–º–æ—â—å—é analyze_strategy_balancing.py
4. **–ï—Å–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –µ—Å—Ç—å:** –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –¥–æ 500k steps
5. **–ï—Å–ª–∏ –Ω–µ—Ç:** –í–Ω–µ–¥—Ä–∏—Ç—å AdaptiveRiskManager

---

*–°–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç 07.02.2026*
