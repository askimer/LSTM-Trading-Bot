# Анализ `train_rl.py` и `trading_environment.py` + план исправлений

Дата: 2026-01-20

## Контекст
Цель: оценить логику RL-обучения и торговой среды, качество кода и основные риски (корректность бухгалтерии, reward shaping, утечки данных, воспроизводимость, валидность оценки).

Файлы:
- `train_rl.py`
- `trading_environment.py`

---

## Основные проблемы (в порядке критичности)

### 1) Утечка данных (data leakage) в нормализации индикаторов
В `preprocess_data()` используется `StandardScaler().fit_transform()` на всём датасете.

Почему это плохо:
- Скейлер “видит будущее”, т.к. mean/std вычисляются по всему периоду, включая будущие относительно текущего шага значения.
- Это завышает качество в обучении/валидации и ухудшает переносимость на real-time.

Где:
- `train_rl.py`: `preprocess_data()` — блок “Правильная нормализация без утечки данных”.

Риск:
- высокий (искажает и обучение, и оценку).

---

### 2) Reward shaping поощряет “торговлю любой ценой”, включая неисполненные действия
В `TradingEnvironment._calculate_reward()` выдаётся большой бонус за любое действие из `[1,2,3,4]`, а не за реальную сделку. Флаг `action_performed` вычисляется, но в награду не входит.

Почему это плохо:
- Агент учится спамить действия ради бонуса, а не оптимизировать прибыль/риск.
- Если действие невозможно (лимиты/недостаточно баланса/маржи), бонус всё равно начисляется, что ломает задачу.

Где:
- `trading_environment.py`: `_calculate_reward()` и `step()` (расчёт `action_performed`).

Риск:
- критический (агент оптимизирует баг/хак награды).

---

### 3) Вероятный двойной учёт комиссий на short
Открытие short считает и записывает комиссию. Закрытие short снова добавляет `open_fee` как будто он ещё не был учтён.

Почему это плохо:
- PnL и reward искажаются, искажается сравнение long/short.
- Агент может “учиться” избегать short по неэкономическим причинам.

Где:
- `trading_environment.py`: `_execute_sell_short()` и `_execute_cover_short()`.

Риск:
- высокий (искажение экономики среды).

---

### 4) Высокий риск некорректной бухгалтерии маржи/short (можны артефакты “денег из воздуха”)
Модель short смешивает “cash”, “proceeds”, “margin_locked” так, что баланс может резко расти при открытии позиции. Да, есть компенсация через `position * price`, но такая бухгалтерия часто даёт неинтуитивные оптимизационные лазейки.

Почему это плохо:
- Агент оптимизирует артефакты состояния/ограничений вместо реальной прибыли.
- Может ломать risk-лимиты (например, по “балансу”).

Где:
- `trading_environment.py`: `_execute_sell_short()` и расчёт портфеля `balance + margin_locked + position * price`.

Риск:
- высокий.

---

### 5) `reset()` всегда стартует с `current_step = 0` (нет рандомизации старта эпизода)

Почему это плохо:
- Эпизоды не разнообразны, высокое переобучение под фиксированный префикс.
- В параллельных средах много одинаковых эпизодов одновременно.

Где:
- `trading_environment.py`: `reset()`.

Риск:
- средний/высокий (качество обучения и обобщение).

---

### 6) `print()` внутри `step()` инициализирует огромный I/O overhead
В среде присутствует отладочный вывод на первых шагах и на каждой исполненной сделке.

Почему это плохо:
- В `SubprocVecEnv` логирование из процессов может резко замедлять обучение и засорять вывод.
- Может маскировать реальные метрики и затруднять дебаг.

Где:
- `trading_environment.py`: `step()` (DEBUG/TRADE prints).

Риск:
- средний (производительность/удобство).

---

### 7) Коллбеки и доступ к метрикам/внутренностям VecEnv выглядят хрупко
`EnhancedTensorBoardCallback` читает `self.locals['dones']`, `self.locals['rewards']`, `episode_lengths` — это не всегда валидно для SB3, особенно с VecEnv.
`EvaluationLoggerCallback` пытается добраться до `eval_env.envs[0].env`, что для `SubprocVecEnv` не является надёжным способом получения состояния.

Почему это плохо:
- Метрики могут писаться неверно или молча не работать.
- Оценка “best model” может быть некорректной.

Где:
- `train_rl.py`: `EnhancedTensorBoardCallback`, `EvaluationLoggerCallback`, настройка `eval_env`.

Риск:
- средний/высокий (валидность логов и оценки).

---

## План исправления (приоритетный)

### Этап A — исправить экономику и награду (критично)
1) **Убрать бонус “за любое торговое действие”** или привязать его строго к `action_performed` и/или к факту открытия/закрытия позиции.
2) Перейти на **reward по дельте портфеля**:
   - \( r_t = \log(\frac{PV_t}{PV_{t-1}}) \) или \( \frac{PV_t - PV_{t-1}}{PV_{t-1}} \)
   - Опционально: штраф за комиссии/просадку/волатильность.
3) Ввести явные понятия:
   - `cash` (свободные средства)
   - `equity` (стоимость счёта)
   - `position_value` и `liability` для short
   - чтобы short не “раздувал” баланс.
4) **Починить комиссии на short**, исключить double-count:
   - хранить “комиссии открытия” или считать комиссии только при фактических исполнениях и один раз.
5) Добавить в `info` поля для дебага экономики:
   - `equity`, `cash`, `position_value`, `fees_step`, `fees_total`, `realized_pnl`, `unrealized_pnl`.

Критерий готовности:
- при нулевой волатильности/нулевых действиях PV стабилен;
- одинаковые комиссии для long/short учитываются симметрично;
- reward коррелирует с ростом equity, а не с частотой действий.

---

### Этап B — убрать утечку данных и сделать корректную оценку (критично)
1) Разделить данные на train/val/test **по времени** (walk-forward).
2) Фитить скейлер только на train и применять на val/test:
   - либо вынести скейлер в пайплайн фичей и сериализовать.
3) Для RL-эпизодов: рассмотреть **онлайн/роллинг нормализацию** (как уже сделано для цены) и/или VecNormalize из SB3.

Критерий готовности:
- на test результаты заметно реалистичнее и устойчивее; нет “магического” улучшения от будущих mean/std.

---

### Этап C — улучшить разнообразие эпизодов и генерализацию (важно)
1) В `reset()` добавить рандомный `start_step` (или выбор случайного окна):
   - фиксировать seed для воспроизводимости.
2) Следить, чтобы эпизод не выходил за границы данных.
3) Оценку проводить на множестве срезов (но честно: без утечек и с отдельным тестом).

Критерий готовности:
- разные эпизоды действительно разные; агент не переучивается на начало ряда.

---

### Этап D — стабилизировать обучение/логирование (важно)
1) Убрать `print()` из `step()` или сделать управляемый логгер с уровнем (debug/info) и выключаемый по умолчанию.
2) Упростить/исправить коллбеки:
   - для episode-метрик лучше опираться на Monitor-логи или корректные хуки SB3.
   - не “распаковывать” `SubprocVecEnv` для чтения внутренних переменных процесса.
3) Добавить стабильные метрики:
   - equity curve, max drawdown, sharpe/sortino, turnover, avg trade PnL, fee ratio.

Критерий готовности:
- TensorBoard и eval-логи согласованы, нет ложных метрик.

---

## Быстрый чек-лист регресса (после правок)
- [ ] Нет утечки данных: скейлер обучен только на train.
- [ ] Reward зависит от equity/PnL, а не от частоты действий.
- [ ] Комиссии и PnL на short считаются один раз и симметрично long.
- [ ] `reset()` рандомизирует окна (при фиксируемом seed).
- [ ] Оценка делается на “невидимом” временном отрезке (test).
- [ ] Логи не спамятся из `step()` в многопроцессной среде.

